# Chatbot_data.          
Chatbot_data_for_Korean v1.0             


## Data description.    

ì¸ê³µë°ì´í„°ì…ë‹ˆë‹¤. ì¼ë¶€ ì´ë³„ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì—ì„œ ë‹¤ìŒì¹´í˜ "ì‚¬ë‘ë³´ë‹¤ ì•„ë¦„ë‹¤ìš´ ì‹¤ì—°( http://cafe116.daum.net/_c21_/home?grpid=1bld )"ì—ì„œ ìì£¼ ë‚˜ì˜¤ëŠ” ì´ì•¼ê¸°ë“¤ì„ ì°¸ê³ í•˜ì—¬ ì œì‘í•˜ì˜€ìŠµë‹ˆë‹¤. 
ê°€ë ¹ "ì´ë³„í•œ ì§€ ì—´í˜(ë˜ëŠ” 100ì¼) ë˜ì—ˆì–´ìš”"ë¼ëŠ” ì§ˆë¬¸ì— ì±—ë´‡ì´ ìœ„ë¡œí•œë‹¤ëŠ” ì·¨ì§€ë¡œ ë‹µë³€ì„ ì‘ì„±í•˜ì˜€ìŠµë‹ˆë‹¤. 


1. ì±—ë´‡ íŠ¸ë ˆì´ë‹ìš© ë¬¸ë‹µ í˜ì–´ 11,876ê°œ           
2. ì¼ìƒë‹¤ë°˜ì‚¬ 0, ì´ë³„(ë¶€ì •) 1, ì‚¬ë‘(ê¸ì •) 2ë¡œ ë ˆì´ë¸”ë§                
                      
                     
## Quick peek.                
                                     
![quick_peek](./data/img/data.png)


## ê´€ë ¨ ì½”ë“œ : [Korean Language Model for Wellness Conversation](https://github.com/nawnoes/WellnessConversationAI?fbclid=IwAR3ZhXYW_DwI2RXP1mbHzvafGXF80QWERa4t6TTz_m2NQug5QwjOwQt6Hvw)
- ì´ ê³³ì— ì €ì¥ëœ ë°ì´í„°ë¥¼ ë§Œë“¤ë©´ì„œ ëˆ„êµ°ê°€ì—ê²Œ ìœ„ë¡œê°€ ë˜ëŠ” ëª¨ë¸ì´ ë‚˜ì˜¤ë©´ ì¢‹ê² ë‹¤ê³  ìƒê°í–ˆì—ˆëŠ”ë° ì œ ìƒê°ë³´ë‹¤ ë” ì˜ ë§Œë“  ëª¨ë¸ì´ ìˆì–´ì„œ ë§í¬ ê±¸ì–´ ë‘¡ë‹ˆë‹¤. ë¶€ì¡±í•œ ë°ì´í„°ì§€ë§Œ ì´ê³³ì— ì €ì¥ëœ ë°ì´í„°ì™€ [AI í—ˆë¸Œ ì •ì‹ ê±´ê°• ìƒë‹´ ë°ì´í„°](http://www.aihub.or.kr/keti_data_board/language_intelligence)  ë¥¼ í† ëŒ€ë¡œ ë§Œë“¤ì—ˆë‹¤ê³  í•©ë‹ˆë‹¤. 
- ì „ì°½ìš± ì™¸(2020), í…ì„œí”Œë¡œ2ì™€ ë¨¸ì‹ ëŸ¬ë‹ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ìì—°ì–´ì²˜ë¦¬, ìœ„í‚¤ë¶ìŠ¤( http://cafe116.daum.net/_c21_/home?grpid=1bld )ì˜ ì±—ë´‡ ë¶€ë¶„ì—ë„ ì´ ë°ì´í„°ê°€ ì‚¬ìš©ëœ ê²ƒìœ¼ë¡œ ì•Œê³  ìˆìŠµë‹ˆë‹¤. ë¹ ë¥´ê²Œ ì±—ë´‡ ë§Œë“¤ê³  ì‹¶ìœ¼ì‹  ë¶„ë“¤ì€ ì°¸ê³ í•˜ì…”ë„ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.
- ë°ì´í„° ë¡œë”ë¥¼ í†µí•œ ë‹¤ìš´ë¡œë“œëŠ” ë‹¤ìŒ ë§í¬ [Korpora: Korean Corpora Archives](https://github.com/ko-nlp/Korpora)ë¥¼ ì°¸ê³ í•˜ì‹œë©´ í¸í•˜ê²Œ ì‚¬ìš©í•˜ì‹¤ ìˆ˜ ìˆì„ ë“¯í•©ë‹ˆë‹¤.



#ì¸ìš©

Youngsook Song.(2018). Chatbot_data_for_Korean v1.0)[Online]. Available : https://github.com/songys/Chatbot_data (downloaded 2022. June. 29.)
---
## 1. Sequence-to-sequence Model(seq2seq)
- ì…ë ¥ëœ ì‹œí€€ìŠ¤ë¡œë¶€í„° ë‹¤ë¥¸ ë„ë©”ì¸ì˜ ì‹œí€€ìŠ¤ë¥¼ ì¶œë ¥í•˜ëŠ” ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‚¬ìš©
- ì±—ë´‡ê³¼ ê¸°ê³„ë²ˆì—­, ë‚´ìš© ìš”ì•½, STT(Speach to Text) ë“±ì—ì„œ í™œìš©
- ë‚´ë¶€ê°€ ë³´ì´ì§€ ì•ŠëŠ” ë¸”ë™ ë°•ìŠ¤ì—ì„œ RNNì˜ ì¡°ë¦½ì„ í†µí•´ êµ¬ì¡°ë¥¼ ìƒì„±
- **ì¸ì½”ë”**ì™€ **ë””ì½”ë”**ë¼ëŠ” ë‘ ê°œì˜ ëª¨ë“ˆë¡œ êµ¬ì„±ë˜ë©° ê°ê°ì˜ RNN ì•„í‚¤í…ì³ë¥¼ ë³´ìœ 
- ê°ê°ì˜ ëª¨ë“ˆì€ RNNì…€ë¡œ êµ¬ì„±ëœ ì•„í‚¤í…ì³ë¡œ ì„±ëŠ¥ì„ ìœ„í•´ ë°”ë‹ë¼ RNNì´ ì•„ë‹ˆë¼ LSTM ì…€ í˜¹ì€ GRU ì…€ë“¤ë¡œ êµ¬ì„±

### Encoder
- ì…ë ¥ ë¬¸ì¥ì˜ ëª¨ë“  ë‹¨ì–´ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì…ë ¥ë°›ì€ ë’¤ì— ë§ˆì§€ë§‰ì— ì´ ëª¨ë“  ë‹¨ì–´ ì •ë³´ë“¤ì„ ì••ì¶•í•´ì„œ **í•˜ë‚˜ì˜ Context vector**ë¥¼ ìƒì„±
- ì¸ì½”ë” RNN ì…€ì˜ ë§ˆì§€ë§‰ ì‹œì ì˜ ì€ë‹‰ìƒíƒœë¥¼ context vectorë¼ í•¨

### Decoder
- ì¸ì½”ë”ë¡œë¶€í„° ì „ì†¡ë°›ì€ Context vectorë¥¼ ë³€í™˜ëœ ë‹¨ì–´ë¡œ í•˜ë‚˜ì”© ìˆœì°¨ì ìœ¼ë¡œ ì¶œë ¥
- ë””ì½”ë”ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ RNNLM(RNN Language Model)
- í…ŒìŠ¤íŠ¸ ê³¼ì •ê³¼ í›ˆë ¨ ê³¼ì •ì˜ ì‘ë™ë°©ì‹ì—ì„œ ì°¨ì´ê°€ ìˆìŒ

1. `í…ŒìŠ¤íŠ¸ ê³¼ì •`
- Context vectorì™€ ì´ˆê¸° ì…ë ¥ìœ¼ë¡œ ë¬¸ì¥ì˜ ì‹œì‘ì„ ì˜ë¯¸í•˜ëŠ” ì‹¬ë³¼ `<sos>`ì„ ì…ë ¥ìœ¼ë¡œ ë°›ìŒ
- ë‹¤ìŒì— ë“±ì¥ í™•ë¥ ì´ ë†’ì€ ë‹¨ì–´ ì˜ˆì¸¡í•˜ê³  ì˜ˆì¸¡ëœ ë‹¨ì–´ë¥¼ ë‹¤ìŒ ì…ë ¥ìœ¼ë¡œ í™œìš©
- ë¬¸ì¥ì˜ ëì„ ì˜ë¯¸í•˜ëŠ” `<eos>`ê°€ ì˜ˆì¸¡ë  ë•Œê¹Œì§€ ìœ„ì˜ ê³¼ì •ì„ ë°˜ë³µ

2. `í›ˆë ¨ ê³¼ì •`
- ë””ì½”ì–´ì—ê²Œ ì¸ì½”ë”ê°€ ë³´ë‚¸ Context vectorì™€ ì‹¤ì œ ì •ë‹µì„ ì…ë ¥ë°›ê³ , ë‚˜ì™€ì•¼ í•˜ëŠ” ì •ë‹µì„ ì•Œë ¤ì£¼ë©´ì„œ í›ˆë ¨(ì‹¤ì œ ì •ë‹µì€ `<sos>`ë¡œ ì‹œì‘í•˜ëŠ” ë¬¸ì¥, ë‚˜ì™€ì•¼ í•˜ëŠ” ì •ë‹µì€ `<eos>`ë¡œ ëë‚˜ëŠ” ë¬¸ì¥)
- í•´ë‹¹ ê³¼ì •ì„ êµì‚¬ ê°•ìš”(teaching forcing)ì´ë¼ê³  í•¨
![ì¸ì½”ë”ì—ì„œ ë””ì½”ë”ê¹Œì§€ì˜ ë¬¸ì¥ ë³€í™˜ íë¦„](./data/img/encoder_to_decoder.png)

Ref.[Blog wikidocs.net - seq2seq](https://wikidocs.net/24996)

---
## Transformer

#### BLEU Scoreê°€ ë‚®ìœ¼ë©´ ì„±ëŠ¥ì´ ì¢‹ì§€ ì•Šë‹¤ëŠ” ì˜ë¯¸ì¸ê°€?
- BLEU Score: 0.0613 (â‰ˆ 6.13%) â†’ ë§¤ìš° ë‚®ì€ ì ìˆ˜
- í‰ê·  Loss: 0.0013 / Perplexity: 1.0013 â†’ ë§¤ìš° ë‚®ìŒ (ì´ìƒì ìœ¼ë¡œ ë‚®ì•„ì•¼ í•˜ì§€ë§Œ, ë„ˆë¬´ ë‚®ìœ¼ë©´ ê³¼ì í•© ê°€ëŠ¥ì„± ìˆìŒ)

#### BLEU Score: 0.0613ì˜ ì˜ë¯¸
- ì¼ë°˜ì ìœ¼ë¡œ ì±—ë´‡ ëª¨ë¸ì—ì„œ BLEU Scoreê°€ 0.2~0.4(20%~40%) ì •ë„ë©´ ì–‘í˜¸í•œ ì„±ëŠ¥
- 0.0613(6.13%)ì€ ëª¨ë¸ì´ ì •ë‹µê³¼ ê±°ì˜ ìœ ì‚¬í•˜ì§€ ì•Šì€ ë¬¸ì¥ì„ ìƒì„±í•˜ê³  ìˆë‹¤ëŠ” ì˜ë¯¸
- BLEU Scoreê°€ ë‚®ë‹¤ë©´ ëª¨ë¸ì´ ì ì ˆí•œ ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í•˜ê³  ìˆì„ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
- ì¦‰, í˜„ì¬ ëª¨ë¸ì˜ ì‘ë‹µ í’ˆì§ˆì´ ì¢‹ì§€ ì•Šë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•  ê°€ëŠ¥ì„±ì´ í¼

#### Lossì™€ PerplexityëŠ” ì¢‹ì€ë° BLEU Scoreê°€ ë‚®ì€ ì´ìœ ?
1. BLEU ScoreëŠ” ë‹¨ìˆœí•œ ë‹¨ì–´ ë§¤ì¹­ ê¸°ë°˜ ì§€í‘œ
    - BLEUëŠ” ë‹¨ìˆœíˆ ìƒì„±ëœ ì‘ë‹µê³¼ ì •ë‹µ ê°„ì˜ n-gram(ë‹¨ì–´ ì¡°í•©) ì¼ì¹˜ë¥¼ ì¸¡ì •
    - ì˜ë¯¸ì ìœ¼ë¡œëŠ” ì ì ˆí•œ ì‘ë‹µì´ë¼ë„ ë‹¨ì–´ê°€ ë‹¤ë¥´ë©´ BLEU Scoreê°€ ë‚®ê²Œ ë‚˜ì˜¬ ìˆ˜ ìˆìŒ
2. Perplexity(PPL)ê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´ ê³¼ì í•© ê°€ëŠ¥ì„±
    - PPLì´ 1.0013ì´ë©´ ëª¨ë¸ì´ ê±°ì˜ ì™„ë²½í•˜ê²Œ ë°ì´í„°ë¥¼ í•™ìŠµí–ˆìŒì„ ì˜ë¯¸
    - í•˜ì§€ë§Œ ê³¼ì í•©ì´ ë°œìƒí•˜ë©´ ìƒˆë¡œìš´ ì…ë ¥ì—ì„œ ì ì ˆí•œ ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í•¨
3. ë°ì´í„°ì…‹ì´ ì¶©ë¶„í•˜ì§€ ì•Šê±°ë‚˜ ì¼ë°˜ì ì¸ ëŒ€í™”ë¥¼ ë°˜ì˜í•˜ì§€ ëª»í•¨
    - í•™ìŠµ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•Šê±°ë‚˜, ë„ˆë¬´ í•œì •ì ì¸ íŒ¨í„´ì„ ê°€ì§€ê³  ìˆìœ¼ë©´ ì„±ëŠ¥ ì €í•˜
    - ëª¨ë¸ì´ ë„ˆë¬´ íŠ¹ì •í•œ íŒ¨í„´ì— ë§ì¶° í•™ìŠµë˜ì—ˆì„ ê°€ëŠ¥ì„± ìˆìŒ
4. ë””ì½”ë”© ë°©ì‹(Beam Search, Temperature, Top-k Sampling) ê°œì„  í•„ìš”
    - í˜„ì¬ Greedy Decoding(argmax ì‚¬ìš©)ìœ¼ë¡œ ë¬¸ì¥ì„ ìƒì„± ì¤‘
    - Beam Search, Top-k Sampling ë“± ê°œì„ ëœ ë°©ë²• ì ìš© ê°€ëŠ¥

#### í•´ê²° ë°©ë²•
1. BLEU Scoreë§Œìœ¼ë¡œ ì„±ëŠ¥ì„ í‰ê°€í•˜ì§€ ì•Šê³ , ì§ì ‘ ëŒ€í™” í…ŒìŠ¤íŠ¸ ìˆ˜í–‰

```python
sample_inputs = ["ì•ˆë…•í•˜ì„¸ìš”?", "ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?", "ë„ˆì˜ ì´ë¦„ì€?", "ë¬´ìŠ¨ ì¼ì„ í•  ìˆ˜ ìˆì–´?"]
for sample in sample_inputs:
    print(f"ğŸ’¬ ì§ˆë¬¸: {sample}")
    print(f"ğŸ¤– ì±—ë´‡: {chatbot_response(transformer, sample, vocab, device)}\n")
```

- BLEU Scoreê°€ ë‚®ì•„ë„ ì˜ë¯¸ì ìœ¼ë¡œ ì ì ˆí•œ ì‘ë‹µì¸ì§€ ì§ì ‘ í™•ì¸ í•„ìš”

2. Greedy Decoding ëŒ€ì‹  Beam Search ì ìš© (ë” ë‹¤ì–‘í•œ ë¬¸ì¥ ìƒì„±)
- í˜„ì¬ëŠ” Greedy Decoding (argmax) ë°©ì‹ìœ¼ë¡œ ë‹¨ìˆœíˆ ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ ë‹¨ì–´ë¥¼ ì„ íƒí•˜ëŠ”ë°, ì´ê²ƒì€ ë‹¨ìˆœí•˜ê³  ì •í™•ë„ê°€ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŒ.
- Beam Search ë˜ëŠ” Top-k Samplingì„ ì‚¬ìš©í•˜ë©´ ë” ë‹¤ì–‘í•œ ë‹µë³€ ìƒì„± ê°€ëŠ¥
- Beam Search ì ìš©

```python
def beam_search_decoding(model, input_tensor, vocab, device, beam_size=3, max_length=50):
    model.eval()
    dec_input = torch.tensor([[vocab["<SOS>"]]], dtype=torch.long).to(device)
    sequences = [(dec_input, 0)]

    for _ in range(max_length):
        all_candidates = []
        for seq, score in sequences:
            with torch.no_grad():
                output = model(input_tensor, seq, training=False)
                topk_probs, topk_indices = torch.topk(output[:, -1, :], beam_size)

            for i in range(beam_size):
                next_token = topk_indices[0, i].item()
                new_seq = torch.cat([seq, torch.tensor([[next_token]], dtype=torch.long).to(device)], dim=1)
                new_score = score + torch.log(topk_probs[0, i].item())  # ë¡œê·¸ í™•ë¥  í•©ì‚°
                all_candidates.append((new_seq, new_score))

        sequences = sorted(all_candidates, key=lambda x: x[1], reverse=True)[:beam_size]  # ìƒìœ„ beam_size ê°œ ì„ íƒ

    response_tokens = sequences[0][0].squeeze(0).tolist()[1:]  # `<SOS>` ì œê±°
    response = [word for word, idx in vocab.items() if idx in response_tokens]
    return " ".join(response)
```

- Beam Searchë¥¼ ì ìš©í•˜ë©´ ë” ë‹¤ì–‘í•œ ë¬¸ì¥ì„ íƒìƒ‰í•˜ì—¬ ìµœì ì˜ ì‘ë‹µ ìƒì„± ê°€ëŠ¥
- BLEU Score ìƒìŠ¹ ê°€ëŠ¥ì„± ìˆìŒ

3. ëª¨ë¸ ì¬í›ˆë ¨ - ê³¼ì í•© ë°©ì§€ (ë°ì´í„° í™•ì¥, Dropout ì¡°ì •)
- Dropoutì„ ì¦ê°€ì‹œì¼œ ê³¼ì í•© ë°©ì§€

```python
class Transformer(nn.Module):
    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, dropout=0.3):  # ğŸ”¥ dropout ì¦ê°€
        super().__init__()
        self.embedding = nn.Embedding(input_vocab_size, d_model)
        self.encoder = TransformerEncoderLayer(d_model, num_heads, dff, dropout)
        self.decoder = TransformerDecoderLayer(d_model, num_heads, dff, dropout)
        self.final_layer = nn.Linear(d_model, target_vocab_size)

    def forward(self, enc_input, dec_input, enc_mask=None, dec_mask=None, training=True):
        enc_input, dec_input = self.embedding(enc_input), self.embedding(dec_input)
        enc_output = self.encoder(enc_input, enc_mask)
        dec_output = self.decoder(dec_input, enc_output, dec_mask, enc_mask, training)
        return self.final_layer(dec_output)
```

- Dropoutì„ 0.1 â†’ 0.3ìœ¼ë¡œ ì¦ê°€í•˜ì—¬ ê³¼ì í•© ë°©ì§€
- PPLì´ ë„ˆë¬´ ë‚®ì€ ë¬¸ì œ í•´ê²° ê°€ëŠ¥ì„± ìˆìŒ

4. ë°ì´í„° ì¦ê°• - Synonym Replacement ì ìš©
- í›ˆë ¨ ë°ì´í„°ê°€ ë„ˆë¬´ í•œì •ì ì´ë¼ë©´ ë°ì´í„° ì¦ê°•(Synonym Replacement, Paraphrasing ë“±)ì„ ì ìš©í•˜ì—¬ í•™ìŠµ ë°ì´í„° ë‹¤ì–‘í™”
- ì˜ˆì œ ì½”ë“œ (NLTK WordNet ì‚¬ìš©):

```python
from nltk.corpus import wordnet

def synonym_replacement(sentence):
    words = sentence.split()
    new_sentence = []
    for word in words:
        synonyms = wordnet.synsets(word)
        if synonyms:
            new_sentence.append(synonyms[0].lemmas()[0].name())  # ì²« ë²ˆì§¸ ë™ì˜ì–´ ì‚¬ìš©
        else:
            new_sentence.append(word)
    return " ".join(new_sentence)

# ì˜ˆì œ ì ìš©
original_sentence = "ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?"
augmented_sentence = synonym_replacement(original_sentence)
print(f"Original: {original_sentence} â†’ Augmented: {augmented_sentence}")
```

- ë°ì´í„° ì¦ê°•ì„ í†µí•´ ëª¨ë¸ì´ ë‹¤ì–‘í•œ í‘œí˜„ì„ í•™ìŠµí•˜ë„ë¡ ìœ ë„
- BLEU Score ê°œì„  ê°€ëŠ¥ì„± ìˆìŒ

#### ê²°ë¡ 
- BLEU Score 0.0613ì€ ë§¤ìš° ë‚®ì€ ê°’ìœ¼ë¡œ, ëª¨ë¸ì˜ ì‘ë‹µ ì„±ëŠ¥ì´ ì¢‹ì§€ ì•Šì„ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
- Lossì™€ Perplexityê°€ ë„ˆë¬´ ë‚®ì€ ê²ƒì€ ê³¼ì í•© ê°€ëŠ¥ì„±ì´ ìˆìœ¼ë©°, Dropoutì„ ì¦ê°€ì‹œì¼œ ë°©ì§€ í•„ìš”
- Beam Searchë¥¼ ì ìš©í•˜ì—¬ ë” ë‹¤ì–‘í•œ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ìˆë„ë¡ ê°œì„  ê°€ëŠ¥
- ë°ì´í„° ì¦ê°•(Synonym Replacement, Paraphrasing)ìœ¼ë¡œ ëª¨ë¸ì´ ë” ë‹¤ì–‘í•œ íŒ¨í„´ì„ í•™ìŠµí•˜ë„ë¡ ìœ ë„
- ë‹¤ìŒ ë‹¨ê³„ëŠ” Beam Search, Dropout ì¡°ì •, ë°ì´í„° ì¦ê°•ì„ ì ìš©í•˜ì—¬ BLEU Scoreë¥¼ ê°œì„ 