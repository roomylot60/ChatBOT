{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhsmf\\anaconda3\\envs\\py39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.legacy.datasets import Multi30k\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f846c66570>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import de_core_news_sm\n",
    "import en_core_web_sm\n",
    "\n",
    "# Load Tokenizing model\n",
    "spacy_en = en_core_web_sm.load()\n",
    "spacy_de = de_core_news_sm.load()\n",
    "\n",
    "# Tokenizer function\n",
    "def tokenize_de(text):\n",
    "    return [token.text for token in spacy_de.tokenizer(text)][::-1]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [token.text for token in spacy_en.tokenizer(text)]\n",
    "\n",
    "src = Field(tokenize=tokenize_de, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "trg = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train, validation, test data\n",
    "# train_data, valid_data, test_data = Multi30k.splits(exts=('.de','.en'), fields=(src,trg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data length: 29000\n",
      "{'src': ['.', 'büsche', 'vieler', 'nähe', 'der', 'in', 'freien', 'im', 'sind', 'männer', 'weiße', 'junge', 'zwei'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}\n",
      "validation data length: 1014\n",
      "{'src': ['lastwagen', 'einen', 'auf', 'baumwolle', 'lädt', 'männern', 'von', 'gruppe', 'eine'], 'trg': ['a', 'group', 'of', 'men', 'are', 'loading', 'cotton', 'onto', 'a', 'truck']}\n",
      "mmt_task1_test2016 data length: 1000\n",
      "{'src': ['.', 'anstarrt', 'etwas', 'der', ',', 'hut', 'orangefarbenen', 'einem', 'mit', 'mann', 'ein'], 'trg': ['a', 'man', 'in', 'an', 'orange', 'hat', 'starring', 'at', 'something', '.']}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from torchtext.legacy.data import Dataset, Field, Example\n",
    "\n",
    "# Tokenizer functions\n",
    "def tokenize_de(text):\n",
    "    return [token.text for token in spacy_de.tokenizer(text)][::-1]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [token.text for token in spacy_en.tokenizer(text)]\n",
    "\n",
    "# Define fields\n",
    "SRC = Field(tokenize=tokenize_de, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "TRG = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "\n",
    "# Directory containing the .tar.gz files\n",
    "data_dir = './.data/multi30k/'\n",
    "\n",
    "# Extract and load data from .tar.gz files\n",
    "def load_data_from_tar(tar_path, src_field, trg_field):\n",
    "    examples = []\n",
    "    \n",
    "    with tarfile.open(tar_path, 'r:gz') as tar:\n",
    "        file_names = [name for name in tar.getnames() if name.endswith('.de') or name.endswith('.en')]\n",
    "        \n",
    "        src_files = [f for f in file_names if f.endswith('.de')]\n",
    "        trg_files = [f for f in file_names if f.endswith('.en')]\n",
    "\n",
    "        for src_file, trg_file in zip(src_files, trg_files):\n",
    "            with tar.extractfile(src_file) as src_f, tar.extractfile(trg_file) as trg_f:\n",
    "                src_lines = src_f.read().decode('utf-8').strip().split('\\n')\n",
    "                trg_lines = trg_f.read().decode('utf-8').strip().split('\\n')\n",
    "                \n",
    "                for src_line, trg_line in zip(src_lines, trg_lines):\n",
    "                    example = Example.fromlist([src_field.preprocess(src_line), trg_field.preprocess(trg_line)], \n",
    "                                               [('src', src_field), ('trg', trg_field)])\n",
    "                    examples.append(example)\n",
    "        \n",
    "        # Convert Example objects to Dataset object\n",
    "        dataset = Dataset(examples, {'src': src_field, 'trg': trg_field})\n",
    "        \n",
    "        return dataset\n",
    "\n",
    "# List of .tar.gz files\n",
    "tar_files = [\n",
    "    'training.tar.gz',\n",
    "    'validation.tar.gz',\n",
    "    'mmt_task1_test2016.tar.gz'\n",
    "]\n",
    "\n",
    "# Load data from each .tar.gz file\n",
    "datasets = {}\n",
    "for tar_file in tar_files:\n",
    "    datasets[tar_file.replace('.tar.gz', '')] = load_data_from_tar(os.path.join(data_dir, tar_file), SRC, TRG)\n",
    "\n",
    "# Check datasets\n",
    "for name, dataset in datasets.items():\n",
    "    print(f\"{name} data length: {len(dataset)}\")\n",
    "    print(vars(dataset.examples[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data length: 29000\n",
      "Valid data length: 1014\n",
      "Test data length: 1000\n"
     ]
    }
   ],
   "source": [
    "train_data = datasets['training']\n",
    "valid_data = datasets['validation']\n",
    "test_data = datasets['mmt_task1_test2016']\n",
    "\n",
    "# Check the sizes of the split datasets\n",
    "print(f\"Train data length: {len(train_data)}\")\n",
    "print(f\"Valid data length: {len(valid_data)}\")\n",
    "print(f\"Test data length: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_freq=2는 2번 이상 등장한 토큰을 출력합니다.\n",
    "# 토큰이 1번만 등장했다면 <unk>로 대체합니다.\n",
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterator 생성\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), batch_size=batch_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # embedding: 입력값을 emd_dim 벡터로 변경\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "\n",
    "        # embedding을 입력받아 hid_dim 크기의 hidden state, cell 출력\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # sre: [src_len, batch_size]\n",
    "\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "\n",
    "        # initial hidden state는 zero tensor\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "\n",
    "        # output: [src_len, batch_size, hid dim * n directions]\n",
    "        # hidden: [n layers * n directions, batch_size, hid dim]\n",
    "        # cell: [n layers * n directions, batch_size, hid dim]\n",
    "\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # content vector를 입력받아 emb_dim 출력\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "\n",
    "        # embedding을 입력받아 hid_dim 크기의 hidden state, cell 출력\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        # input: [batch_size]\n",
    "        # hidden: [n layers * n directions, batch_size, hid dim]\n",
    "        # cell: [n layers * n directions, batch_size, hid dim]\n",
    "\n",
    "        input = input.unsqueeze(0) # input: [1, batch_size], 첫번째 input은 <SOS>\n",
    "\n",
    "        embedded = self.dropout(self.embedding(input)) # [1, batch_size, emd dim]\n",
    "\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        # output: [seq len, batch_size, hid dim * n directions]\n",
    "        # hidden: [n layers * n directions, batch size, hid dim]\n",
    "        # cell: [n layers * n directions, batch size, hid dim]\n",
    "\n",
    "        prediction = self.fc_out(output.squeeze(0)) # [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq2Seq\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "        # encoder와 decoder의 hid_dim이 일치하지 않는 경우 에러메세지\n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            'Hidden dimensions of encoder decoder must be equal'\n",
    "        # encoder와 decoder의 hid_dim이 일치하지 않는 경우 에러메세지\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            'Encoder and decoder must have equal number of layers'\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # src: [src len, batch size]\n",
    "        # trg: [trg len, batch size]\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0] # 타겟 토큰 길이 얻기\n",
    "        trg_vocab_size = self.decoder.output_dim # context vector의 차원\n",
    "\n",
    "        # decoder의 output을 저장하기 위한 tensor\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        # initial hidden state\n",
    "        hidden, cell = self.encoder(src)\n",
    "\n",
    "        # 첫 번째 입력값 <sos> 토큰\n",
    "        input = trg[0,:]\n",
    "\n",
    "        for t in range(1,trg_len): # <eos> 제외하고 trg_len-1 만큼 반복\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "\n",
    "            # prediction 저장\n",
    "            outputs[t] = output\n",
    "\n",
    "            # teacher forcing을 사용할지, 말지 결정\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            # 가장 높은 확률을 갖은 값 얻기\n",
    "            top1 = output.argmax(1)\n",
    "\n",
    "            # teacher forcing의 경우에 다음 lstm에 target token 입력\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터 지정\n",
    "input_dim = len(SRC.vocab)\n",
    "output_dim = len(TRG.vocab)\n",
    "enc_emb_dim = 256 # 임베딩 차원\n",
    "dec_emb_dim = 256\n",
    "hid_dim = 512 # hidden state 차원\n",
    "n_layers = 2\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "enc = Encoder(input_dim, enc_emb_dim, hid_dim, n_layers, enc_dropout)\n",
    "dec = Decoder(output_dim, dec_emb_dim, hid_dim, n_layers, dec_dropout)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(7853, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(5893, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=512, out_features=5893, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가중치 초기화\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 13,898,501 trainableparameters\n"
     ]
    }
   ],
   "source": [
    "# 모델의 학습가능한 파라미터 수 측정\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainableparameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# loss function\n",
    "# pad에 해당하는 index는 무시합니다.\n",
    "trg_pad_idx = TRG.vocab.stoi[TRG.pad_token]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=trg_pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습을 위한 함수\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src,trg) # [trg len, batch size, output dim]\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim) # loss 계산을 위해 1d로 변경\n",
    "        trg = trg[1:].view(-1) # loss 계산을 위해 1d로 변경\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "\n",
    "        # 기울기 clip\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation function\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            # output: [trg len, batch size, output dim]\n",
    "            output = model(src, trg, 0) # teacher forcing off\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim) # [(trg len -1) * batch size, output dim]\n",
    "            trg = trg[1:].view(-1) # [(trg len -1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to count training time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 시작\n",
    "num_epochs = 10\n",
    "clip = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "   \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, clip)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best val loss일 때의 가중치를 불러옵니다.\n",
    "model.load_state_dict(torch.load('tut1-model.pt'))\n",
    "\n",
    "# test loss를 측정합니다.\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
