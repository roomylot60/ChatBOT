{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seqence to Sequence Model\n",
    "- 하나의 텍스트 문장이 입력으로 들어오면 하나의 텍스트 문장을 출력하는 구조\n",
    "- Encoder -> (Context Vector) -> Decoder\n",
    "    - 입력된 데이터를 Encoder를 거쳐 Context Vector라는 하나의 함축된 데이터로 변경(Encoder 마지막의 은닉계층에서 압축과정 중 손실 발생 -> Attention Mechanism)\n",
    "    - 벡터가 Decoder를 거쳐 Label에 맞게 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'data/ChatbotData.csv'\n",
    "VOCAB_PATH = 'data/vocabulary.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, outputs = load_data(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chr2idx, idx2chr, wocab_size = load_vocabulary(PATH, VOCAB_PATH, tokenize_as_morph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_inputs, input_seq_len = enc_processing(inputs, chr2idx, tokenize_as_morph=False)\n",
    "index_outputs, output_seq_len =dec_output_processing(outputs, chr2idx, tokenize_as_morph=False)\n",
    "index_targets = dec_target_processing(outputs, chr2idx, tokenize_as_morph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary form\n",
    "data_configs = {}\n",
    "data_configs['chr2idx'] = chr2idx\n",
    "data_configs['idx2chr'] = idx2chr\n",
    "data_configs['vocab_size'] = vocab_size\n",
    "data_configs['pad_symbol'] = PAD\n",
    "data_configs['std_symbol'] = STD\n",
    "data_configs['end_symbol'] = END\n",
    "data_configs['unk_symbol'] = UNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save preprocessed data in directory\n",
    "DATA_IN_PATH = './data/'\n",
    "TRAIN_INPUTS = 'train_inputs.npy'\n",
    "TRAIN_OUTPUTS = 'train_outputs.npy'\n",
    "TRAIN_TARGETS = 'train_targets.npy'\n",
    "DATA_CONFIGS = 'data_configs.json'\n",
    "\n",
    "np.save(open(DATA_IN_PATH + TRAIN_INPUTS, 'wb'), index_inputs)\n",
    "np.save(open(DATA_IN_PATH + TRAIN_OUTPUTS, 'wb'), index_outputs)\n",
    "np.save(open(DATA_IN_PATH + TRAIN_TARGETS, 'wb'), index_targets)\n",
    "\n",
    "json.dump(data_configs, open(DATA_PATH + DATA_CONFIGS, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9271e414be5e055cabef0148537efe95905a2cbc3a51060d18455594802bc000"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
