{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seqence to Sequence Model\n",
    "- 하나의 텍스트 문장이 입력으로 들어오면 하나의 텍스트 문장을 출력하는 구조\n",
    "- Encoder -> (Context Vector) -> Decoder\n",
    "    - 입력된 데이터를 Encoder를 거쳐 Context Vector라는 하나의 함축된 데이터로 변경(Encoder 마지막의 은닉계층에서 압축과정 중 손실 발생 -> Attention Mechanism)\n",
    "    - 벡터가 Decoder를 거쳐 Label에 맞게 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install matplot\n",
    "%pip install wordcloud\n",
    "\n",
    "%pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module import\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings(특수문자 제거, 특수 토큰 정의)\n",
    "FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "PAD = \"<PAD>\"\n",
    "STD = \"<SOS>\"\n",
    "END = \"<END>\"\n",
    "UNK = \"<UNK>\"\n",
    "\n",
    "PAD_INDEX = 0\n",
    "STD_INDEX = 1\n",
    "END_INDEX = 2\n",
    "UNK_INDEX = 3\n",
    "\n",
    "MARKER =[PAD, STD, END, UNK]\n",
    "CHANGE_FILTER = re.compile(FILTERS)\n",
    "\n",
    "MAX_SEQ = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Load\n",
    "def load_data(path):\n",
    "    data_df = pd.read_csv(path, header=0)\n",
    "    qst, ans = list(data_df['Q']), list(data_df['A'])\n",
    "    \n",
    "    return qst, ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특수문자 제거 및 전체 문장에 대한 단어 사전 리스트로 반환\n",
    "def data_tokenizer(data):\n",
    "    # array to append tokenized data\n",
    "    words = []\n",
    "    for sentence in data:\n",
    "        # FILTERS 내의 값들을 정규화 표현식을 통해서 모두 문자로 변환\n",
    "        sentence = re.sub(CHANGE_FILTER, \"\", sentence)\n",
    "        for word in sentence.split():\n",
    "            words.append(word)\n",
    "    # tokeninzing, 정규화를 거친 값들을 반환\n",
    "    return [word for word in words if word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 기준 txt_tokenizing 후 각 문장을 형태소들의 리스트로 반환\n",
    "def prepro_like_morpheme(data):\n",
    "    morph_analyzer = Okt()\n",
    "    result_data = []\n",
    "    for seq in tqdm(data):\n",
    "        morphemic_seq = \" \".join(morph_analyzer.morphs(seq.replace(' ', '')))\n",
    "        result_data.append(morphemic_seq)\n",
    "    return result_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Word Dictionary\n",
    "# Return the values (word:index), (index:word), (num_of_total_word)\n",
    "def load_vocabulary(path, vocab_path, tokenize_as_morph=False):\n",
    "    # List for dictionary\n",
    "    vocab_list = []\n",
    "    # Checking the file presence on the path\n",
    "    if not os.path.exists(vocab_path):\n",
    "        if (os.path.exists(path)):\n",
    "            # load data\n",
    "            data_df = pd.read_csv(path, encoding='utf-8')\n",
    "            # array for questions and answers\n",
    "            qst, ans = list(data_df['Q']), list(data_df['A'])\n",
    "            if tokenize_as_morph:\n",
    "                # 형태소에 따른 tokeninzing\n",
    "                qst = prepro_like_morpheme(qst)\n",
    "                ans = prepro_like_morpheme(ans)\n",
    "            data = []\n",
    "            # .extend(전달값) : 전달값(iterable)을 하나씩 분해해서 리스트에 각 원소로 저장(!! .append()와 구분)\n",
    "            data.extend(qst)\n",
    "            data.extend(ans)\n",
    "            \n",
    "            words = data_tokenizer(data)\n",
    "            # 중복 단어 제거를 위해 set\n",
    "            words = list(set(words))\n",
    "            # MARKER를 사전에 추가\n",
    "            # 순서대로 넣기 위해서 인덱스 0 에 추가\n",
    "            # PAD = \"<PADDING>\"\n",
    "            # STD = \"<START>\"\n",
    "            # END = \"<END>\"\n",
    "            # UNK = \"<UNKOWN>\"\n",
    "            words[:0] = MARKER\n",
    "        \n",
    "        # 리스트로 된 단어 사전을 vocabulary_file에 저장\n",
    "        # 각 단어는 개행 문자('\\n')를 오른쪽에 달고 저장\n",
    "        with open(vocab_path, 'w', encoding='utf-8') as vocabulary_file:\n",
    "            for word in words:\n",
    "                vocabulary_file.write(word + '\\n')\n",
    "    \n",
    "    # 사전 파일이 존재하면 파일을 불러스 리스트에 추가\n",
    "    # 각 개행 문자를 .strip()으로 제거 후 사전 리스트로 저장\n",
    "    with open(vocab_path, 'w', encoding='utf-8') as vocabulary_file:\n",
    "        for line in vocabulary_file:\n",
    "            vocab_list.append(line.strip())\n",
    "    \n",
    "    # 리스트 내용을 key:value 형태의 dictionary 구조로 생성\n",
    "    chr2idx, idx2chr = make_vocabulary(vocab_list)\n",
    "    # 두 가지 형태의 key, value 형태를 리턴\n",
    "    return chr2idx, idx2chr, len(chr2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어:인덱스, 인덱스:단어, 단어 개수 사전 반환 함수\n",
    "def make_vocabulary(vocab_list):\n",
    "    chr2idx = {chr:idx for idx, chr in enumerate(vocab_list)}\n",
    "    idx2chr = {idx:chr for idx, chr in enumerate(vocab_list)}\n",
    "    return chr2idx, idx2chr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9271e414be5e055cabef0148537efe95905a2cbc3a51060d18455594802bc000"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
