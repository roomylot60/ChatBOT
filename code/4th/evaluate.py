# âœ… evaluate.py (KoBERT + Transformer Decoder í‰ê°€)
import os
import json
import torch
import torch.nn as nn
from transformers import AutoTokenizer, BertModel
from train import TransformerDecoderModel
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from rouge_score import rouge_scorer
from tqdm import tqdm

# ì„¤ì •
MODEL_PATH = "models/layer_6_epoch_12_kobert.pt"
CORPUS_PATH = "data/output/corpus_valid.json"
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# âœ… í† í¬ë‚˜ì´ì € ë° KoBERT ì¸ì½”ë”
tokenizer = AutoTokenizer.from_pretrained("monologg/kobert", use_fast=False, trust_remote_code=True)
pad_id = tokenizer.pad_token_id
vocab_size = tokenizer.vocab_size
kobert_enc = BertModel.from_pretrained("monologg/kobert", trust_remote_code=True)

# âœ… í‰ê°€ìš© ë””ì½”ë” ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
model = TransformerDecoderModel().to(DEVICE)
model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))
model.eval()
print("[INFO] âœ… í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

# âœ… ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ
with open(CORPUS_PATH, "r", encoding="utf-8") as f:
    data = json.load(f)
inputs = data["input"]
targets = data["target"]

# âœ… ìƒ˜í”Œ ìƒì„± í•¨ìˆ˜
def generate_response(question_ids):
    input_ids = torch.tensor([question_ids]).to(DEVICE)
    attention_mask = (input_ids != pad_id).long()
    generated = [tokenizer.cls_token_id]

    for _ in range(64):
        dec_input = torch.tensor([generated]).to(DEVICE)
        output = model(input_ids, attention_mask, dec_input)
        next_token = output.argmax(-1)[0, -1].item()
        if next_token == tokenizer.sep_token_id or len(generated) >= 64:
            break
        generated.append(next_token)
    return tokenizer.decode(generated[1:], skip_special_tokens=True)

# âœ… í‰ê°€ ì§€í‘œ ì´ˆê¸°í™”
smooth = SmoothingFunction().method1
scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)

bleu_scores = []
rouge1_scores = []
rougeL_scores = []
acc = 0
ppl_total = 0
loss_fn = nn.CrossEntropyLoss(ignore_index=pad_id)

# âœ… ì „ì²´ í‰ê°€ ë£¨í”„
print("\n[INFO] ğŸ§ª ì„±ëŠ¥ í‰ê°€ ì‹œì‘...\n")
for i in tqdm(range(len(inputs)), desc="Evaluating"):
    input_ids = inputs[i]["input_ids"]
    target_ids = targets[i]["input_ids"]

    question = tokenizer.decode(input_ids, skip_special_tokens=True)
    answer_gt = tokenizer.decode(target_ids, skip_special_tokens=True)
    answer_gen = generate_response(input_ids)

    # BLEU, ROUGE
    bleu = sentence_bleu([answer_gt.split()], answer_gen.split(), smoothing_function=smooth)
    rouge = scorer.score(answer_gt, answer_gen)

    bleu_scores.append(bleu)
    rouge1_scores.append(rouge['rouge1'].fmeasure)
    rougeL_scores.append(rouge['rougeL'].fmeasure)

    if answer_gt.strip() == answer_gen.strip():
        acc += 1

    # Perplexity
    input_tensor = torch.tensor([input_ids]).to(DEVICE)
    tgt_tensor = torch.tensor([target_ids]).to(DEVICE)
    attn_mask = (input_tensor != pad_id).long()
    dec_input = tgt_tensor[:, :-1]
    dec_target = tgt_tensor[:, 1:]
    with torch.no_grad():
        output = model(input_tensor, attn_mask, dec_input)
        loss = loss_fn(output.view(-1, vocab_size), dec_target.reshape(-1))
        ppl_total += torch.exp(loss).item()

# âœ… ê²°ê³¼ ì¶œë ¥
print("\n[âœ… ì„±ëŠ¥ í‰ê°€ ê²°ê³¼]")
print(f"BLEU Score   : {sum(bleu_scores) / len(bleu_scores):.4f}")
print(f"ROUGE-1 F1   : {sum(rouge1_scores) / len(rouge1_scores):.4f}")
print(f"ROUGE-L F1   : {sum(rougeL_scores) / len(rougeL_scores):.4f}")
print(f"Accuracy     : {acc / len(inputs):.4f}")
print(f"Perplexity   : {ppl_total / len(inputs):.4f}")

# âœ… ìƒ˜í”Œ ì¶œë ¥
print("\n[INFO] ğŸ’¬ ìƒ˜í”Œ ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±\n")
for i in range(5):
    input_ids = inputs[i]["input_ids"]
    target_ids = targets[i]["input_ids"]

    question = tokenizer.decode(input_ids, skip_special_tokens=True)
    answer_gt = tokenizer.decode(target_ids, skip_special_tokens=True)
    answer_gen = generate_response(input_ids)

    print(f"ğŸŸ¨ ì§ˆë¬¸: {question}")
    print(f"ğŸŸ¦ ì‹¤ì œ ì‘ë‹µ: {answer_gt}")
    print(f"ğŸŸ§ ìƒì„± ì‘ë‹µ: {answer_gen}")
    print("-" * 80)
