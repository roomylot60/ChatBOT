# âœ… preprocess.py
import os
import re
import pandas as pd
import zipfile
import json
from transformers import AutoTokenizer
from tqdm import tqdm

# âœ… ì„¤ì •
MAX_LEN = 64 # ë‹µë³€ì„ ë„ˆë¬´ ë§ì´ ìë¥´ëŠ” ê²ƒ ê°™ì•„ì„œ 128ë¡œ ìˆ˜ì •í• ê¹Œ ê³ ë¯¼ ì¤‘
DATA_DIR = "data/Interview/Training/Labeling"
TOKENIZER_PATH = os.path.join("data/output", "tokenizer")
CORPUS_PATH = os.path.join("data/output", "corpus_tokenized.json")

# âœ… ë°ì´í„° ì •ì œ
def clean_json_string(raw):
    # í—ˆìš©ë˜ì§€ ì•ŠëŠ” ì œì–´ ë¬¸ì ì œê±°
    return re.sub(r'[\x00-\x1f\x7f]', '', raw)

# âœ… ë°ì´í„° ì¶”ì¶œ
def extract_qa_pairs():
    qa_pairs = []

    for root, _, files in os.walk(DATA_DIR):
        for file in files:
            if not file.endswith('.zip'):
                continue

            zip_path = os.path.join(root, file)
            print(f"[INFO] ğŸ” ì²˜ë¦¬ ì¤‘: {zip_path}")

            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                for json_file in sorted(zip_ref.namelist()):
                    if not json_file.endswith('.json'):
                        continue

                    try:
                        with zip_ref.open(json_file) as f:
                            raw = f.read().decode("utf-8", errors="ignore")
                            cleaned = clean_json_string(raw)
                            data = json.loads(cleaned)
                            df = pd.DataFrame(data)

                            question_raw = df.at['question', 'dataSet']['raw']['text']
                            answer_raw = df.at['answer', 'dataSet']['raw']['text']

                            if question_raw.strip() and answer_raw.strip():
                                qa_pairs.append((question_raw.strip(), answer_raw.strip()))
                    except Exception as e:
                        print(f"[WARN] â— {file}/{json_file} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

    print(f"[INFO] âœ… ì¶”ì¶œëœ QA ìŒ ìˆ˜: {len(qa_pairs)}")
    return qa_pairs

# âœ… ë°ì´í„° ì €ì¥
def save_qa_pairs(qa_pairs):
    with open("data/output/processed_interview_data.json", "w", encoding="utf-8") as f:
        json.dump(qa_pairs, f, ensure_ascii=False, indent=2)
        print("[INFO] ì €ì¥ ì™„ë£Œ: processed_interview_data.json")

# âœ… KoBERT í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained("monologg/kobert", use_fast=False)
print("[INFO] âœ… KoBERT í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")
print(f"[INFO] ğŸ”¢ Vocab í¬ê¸°: {tokenizer.vocab_size}")
print(f"[INFO] ğŸ”  ì˜ˆì‹œ í† í°í™”: {tokenizer.tokenize('ê¸°ë¶„ì´ ì•ˆ ì¢‹ì•„')}")
print(f"[INFO] ğŸ”¤ CLS ID: {tokenizer.cls_token_id}, SEP ID: {tokenizer.sep_token_id}, PAD ID: {tokenizer.pad_token_id}")

# âœ… KoBERT í† í¬ë‚˜ì´ì§•
def tokenize_and_encode(q_list, a_list):
    input_encodings = []
    target_encodings = []
    for q, a in tqdm(zip(q_list, a_list), total=len(q_list), desc="[TOKENIZING]"):
        q_ids = tokenizer.encode(q, max_length=MAX_LEN, padding="max_length", truncation=True)
        a_ids = tokenizer.encode(a, max_length=MAX_LEN, padding="max_length", truncation=True)
        input_encodings.append({"input_ids": q_ids})
        target_encodings.append({"input_ids": a_ids})
    return input_encodings, target_encodings

# âœ… ì „ì²˜ë¦¬ ì‹¤í–‰
def save_preprocessed_data():
    qa_pairs = extract_qa_pairs()
    q_list, a_list = zip(*qa_pairs)

    if len(q_list) == 0:
        print("[ERROR] ìˆ˜ì§‘ëœ ë¬¸ì¥ì´ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return

    print("[INFO] âœ¨ í† í¬ë‚˜ì´ì§• ë° ì¸ì½”ë”© ì‹œì‘...")
    input_encodings, target_encodings = tokenize_and_encode(q_list, a_list)

    print(f"[INFO] ğŸ” ìƒ˜í”Œ QnA:\nQ: {q_list[0]}\nA: {a_list[0]}")
    print(f"[INFO] ğŸ” ìƒ˜í”Œ ì¸ì½”ë”©:\nQ: {input_encodings[0]['input_ids']}\nA: {target_encodings[0]['input_ids']}")

    os.makedirs(DATA_DIR, exist_ok=True)
    with open(CORPUS_PATH, "w", encoding="utf-8") as f:
        json.dump({"input": input_encodings, "target": target_encodings}, f, ensure_ascii=False, indent=2)

    tokenizer.save_vocabulary(TOKENIZER_PATH)
    print(f"[âœ… ì™„ë£Œ] ì €ì¥ëœ ìŒ: {len(input_encodings)}ê°œ")
    print(f"[âœ… ì™„ë£Œ] í† í¬ë‚˜ì´ì € ì €ì¥ ê²½ë¡œ: {TOKENIZER_PATH}")
    print(f"[âœ… ì™„ë£Œ] ì½”í¼ìŠ¤ ì €ì¥ ê²½ë¡œ: {CORPUS_PATH}")

if __name__ == "__main__":
    save_preprocessed_data()
