# âœ… preprocess.py
import os
import re
import pandas as pd
import zipfile
import json
from transformers import AutoTokenizer
from tqdm import tqdm

# âœ… ì„¤ì •
MAX_LEN = 64 # ë‹µë³€ì„ ë„ˆë¬´ ë§ì´ ìë¥´ëŠ” ê²ƒ ê°™ì•„ì„œ 128ë¡œ ìˆ˜ì •í• ê¹Œ ê³ ë¯¼ ì¤‘
TOKENIZER_PATH = "data/tokenizer"
TRAIN_DIR = "data/Interview/Training/Labeling"
VALID_DIR = "data/Interview/Validation/Labeling"
CORPUS_TRAIN = "data/output/corpus_train.json"
CORPUS_VALID = "data/output/corpus_valid.json"

# âœ… ë°ì´í„° ì •ì œ
def clean_json_string(raw):
    # í—ˆìš©ë˜ì§€ ì•ŠëŠ” ì œì–´ ë¬¸ì ì œê±°
    return re.sub(r'[\x00-\x1f\x7f]', '', raw)

# âœ… ë°ì´í„° ì¶”ì¶œ
def extract_qa_pairs(zip_dir):
    qa_pairs = []
    for root, _, files in os.walk(zip_dir):
        for file in files:
            if not file.endswith('.zip'):
                continue
            zip_path = os.path.join(root, file)
            print(f"[INFO] ğŸ” ì²˜ë¦¬ ì¤‘: {zip_path}")

            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                for json_file in sorted(zip_ref.namelist()):
                    if not json_file.endswith('.json'):
                        continue

                    try:
                        with zip_ref.open(json_file) as f:
                            raw = f.read().decode("utf-8", errors="ignore")
                            cleaned = clean_json_string(raw)
                            data = json.loads(cleaned)
                            df = pd.DataFrame(data)

                            question_raw = df.at['question', 'dataSet']['raw']['text']
                            answer_raw = df.at['answer', 'dataSet']['raw']['text']

                            if question_raw.strip() and answer_raw.strip():
                                qa_pairs.append((question_raw.strip(), answer_raw.strip()))
                    except Exception as e:
                        print(f"[WARN] â— {file}/{json_file} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

    print(f"[INFO] âœ… ì¶”ì¶œëœ QA ìŒ ìˆ˜: {len(qa_pairs)}")
    return qa_pairs

# âœ… KoBERT í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained("monologg/kobert", use_fast=False, trust_remote_code=True)
print("[INFO] âœ… KoBERT í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")
print(f"[INFO] ğŸ”¢ Vocab í¬ê¸°: {tokenizer.vocab_size}")
print(f"[INFO] ğŸ”  ì˜ˆì‹œ í† í°í™”: {tokenizer.tokenize('ê¸°ë¶„ì´ ì•ˆ ì¢‹ì•„')}")
print(f"[INFO] ğŸ”¤ CLS ID: {tokenizer.cls_token_id}, SEP ID: {tokenizer.sep_token_id}, PAD ID: {tokenizer.pad_token_id}")

# âœ… KoBERT í† í¬ë‚˜ì´ì§•
def tokenize_and_encode(q_list, a_list):
    input_encodings = []
    target_encodings = []
    for q, a in tqdm(zip(q_list, a_list), total=len(q_list), desc="[TOKENIZING]"):
        q_ids = tokenizer.encode(q, max_length=MAX_LEN, padding="max_length", truncation=True)
        a_ids = tokenizer.encode(a, max_length=MAX_LEN, padding="max_length", truncation=True)
        input_encodings.append({"input_ids": q_ids})
        target_encodings.append({"input_ids": a_ids})
    return input_encodings, target_encodings

# âœ… ì „ì²˜ë¦¬ ë°ì´í„° ì €ì¥
def save_dataset(zip_dir, output_path, label=""):
    qa_pairs = extract_qa_pairs(zip_dir)

    if not qa_pairs:
        print(f"[ERROR] âŒ {label} ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return

    q_list, a_list = zip(*qa_pairs)

    print(f"[INFO] âœ¨ {label} í† í¬ë‚˜ì´ì§• ë° ì¸ì½”ë”© ì‹œì‘...")
    input_encodings, target_encodings = tokenize_and_encode(q_list, a_list)

    # ë””ë²„ê¹…ìš© ìƒ˜í”Œ ì¶œë ¥
    print(f"[INFO] ğŸ” {label} ìƒ˜í”Œ QnA:\nQ: {q_list[0]}\nA: {a_list[0]}")
    print(f"[INFO] ğŸ” {label} ìƒ˜í”Œ ì¸ì½”ë”©:\nQ: {input_encodings[0]['input_ids']}\nA: {target_encodings[0]['input_ids']}")

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump({"input": input_encodings, "target": target_encodings}, f, ensure_ascii=False, indent=2)

    print(f"[âœ…] {label} ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_path} (ì´ {len(input_encodings)} ìŒ)")

if __name__ == "__main__":
    tokenizer.save_vocabulary(TOKENIZER_PATH)
    save_dataset(TRAIN_DIR, CORPUS_TRAIN, label="Train")
    save_dataset(VALID_DIR, CORPUS_VALID, label="Validation")
