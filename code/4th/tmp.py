# import os
# import json
# import torch
# import torch.nn as nn
# from tqdm import tqdm
# from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
# from rouge_score import rouge_scorer
# from KoBERT import KoBERT_Transformer
# from preprocess import CORPUS_PATH, TOKENIZER_PATH
# from tokenization_kobert import KoBertTokenizer
# # ì„¤ì •
# MAX_LEN = 64
# DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# print(f"[INFO] Device: {DEVICE}")

# tokenizer = KoBertTokenizer.from_pretrained("monologg/kobert")

# # ğŸ”¹ ëª¨ë¸ ë¡œë“œ
# model = KoBERT_Transformer(decoder_vocab_size=tokenizer.vocab_size).to(DEVICE)
# model.load_state_dict(torch.load("models/kobert_chatbot.pt", map_location=DEVICE))
# model.eval()

# def generate_response(input_text):
#     model.eval()
#     # ğŸ”¹ ì…ë ¥ ë¬¸ì¥ ë° í† í° í™•ì¸
#     print("[INPUT]", input_text)
#     token_ids = tokenizer.encode(input_text)
#     print("[ENCODED TOKEN IDS]", token_ids)
#     print("[ENCODED TOKENS]", tokenizer.convert_ids_to_tokens(token_ids))

#     enc = tokenizer.encode_plus(
#         input_text,
#         return_tensors="pt",
#         truncation=True,
#         max_length=MAX_LEN,
#         padding="max_length"
#     )

#     input_tensor = enc["input_ids"].to(DEVICE)
#     enc_mask = enc["attention_mask"].to(DEVICE)

#     START_TOKEN = tokenizer.cls_token_id
#     SEP_TOKEN = tokenizer.sep_token_id

#     dec_input = torch.tensor([[START_TOKEN]], dtype=torch.long).to(DEVICE)
#     result = []

#     for _ in range(MAX_LEN):
#         with torch.no_grad():
#             output = model(input_tensor, enc_mask, dec_input)
#         next_token = output.argmax(-1)[:, -1].item()
#         print(f"[DEBUG] next_token: {next_token} â†’ {tokenizer.convert_ids_to_tokens([next_token])}")
#         if next_token == SEP_TOKEN and len(result) > 1:
#             break
#         result.append(next_token)
#         dec_input = torch.cat([dec_input, torch.tensor([[next_token]]).to(DEVICE)], dim=1)

#     return tokenizer.decode(result, skip_special_tokens=True)

# print("\n[ë””ì½”ë”© í…ŒìŠ¤íŠ¸]")
# print("[MODEL PARAMS]", sum(p.numel() for p in model.parameters()))
# tokens = tokenizer.tokenize("ê¸°ë¶„ì´ ì•ˆ ì¢‹ì•„")
# print(tokens)
# ids = tokenizer.convert_tokens_to_ids(tokens)
# print(ids)
# response = generate_response("ê¸°ë¶„ì´ ì•ˆ ì¢‹ì•„")  # ê°ì •ì´ ëšœë ·í•œ ë¬¸ì¥
# print(f"ì‘ë‹µ: '{response}'")  # â† ì—¬ê¸° ê³µë€ì´ë©´ ë°”ë¡œ ìœ„ ë””ë²„ê¹… í¬ì¸íŠ¸ë¡œ ì ê²€

import os
import zipfile
import json
from tqdm import tqdm

def process_zip_files(directory, zip_prefix, output_file):
    """
    ì§€ì •ëœ ë””ë ‰í† ë¦¬ì—ì„œ íŠ¹ì • ì ‘ë‘ì‚¬ë¥¼ ê°€ì§„ ZIP íŒŒì¼ë“¤ì„ ì²˜ë¦¬í•˜ì—¬ (ì§ˆë¬¸, ë‹µë³€) ìŒì„ ì¶”ì¶œí•©ë‹ˆë‹¤.

    Parameters:
    - directory (str): ZIP íŒŒì¼ë“¤ì´ ìœ„ì¹˜í•œ ë””ë ‰í† ë¦¬ ê²½ë¡œ
    - zip_prefix (str): ì„ íƒí•  ZIP íŒŒì¼ë“¤ì˜ ì ‘ë‘ì‚¬
    - output_file (str): ì¶”ì¶œëœ (ì§ˆë¬¸, ë‹µë³€) ìŒì„ ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
    """
    qa_pairs = []

    # ë””ë ‰í† ë¦¬ ë‚´ì˜ ëª¨ë“  íŒŒì¼ì— ëŒ€í•´ ë°˜ë³µ
    for filename in tqdm(os.listdir(directory), desc="Processing ZIP files"):
        if filename.startswith(zip_prefix) and filename.endswith('.zip'):
            zip_path = os.path.join(directory, filename)
            try:
                with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                    # ZIP íŒŒì¼ì˜ ë¬´ê²°ì„± ê²€ì‚¬
                    if zip_ref.testzip() is not None:
                        print(f"Warning: {zip_path} is corrupted.")
                        continue

                    # ZIP íŒŒì¼ ë‚´ì˜ ëª¨ë“  JSON íŒŒì¼ì— ëŒ€í•´ ë°˜ë³µ
                    for json_file in zip_ref.namelist():
                        if json_file.endswith('.json'):
                            with zip_ref.open(json_file) as f:
                                try:
                                    # JSON íŒŒì¼ì˜ ì¸ì½”ë”©ì„ ê³ ë ¤í•˜ì—¬ ì½ê¸°
                                    data = json.load(f)
                                    # ë°ì´í„° êµ¬ì¡°ì— ë”°ë¼ 'utterance' í‚¤ë¥¼ í™•ì¸
                                    if 'utterance' in data:
                                        utterances = data['utterance']
                                        for i in range(len(utterances) - 1):
                                            question = utterances[i]['text'].strip()
                                            answer = utterances[i + 1]['text'].strip()
                                            if question and answer:
                                                qa_pairs.append({"question": question, "answer": answer})
                                except json.JSONDecodeError:
                                    print(f"Error decoding JSON from file: {json_file} in {zip_path}")
            except zipfile.BadZipFile:
                print(f"Error: {zip_path} is not a valid ZIP file.")

    # ì¶”ì¶œëœ (ì§ˆë¬¸, ë‹µë³€) ìŒì„ JSONL í˜•ì‹ìœ¼ë¡œ ì €ì¥
    with open(output_file, 'w', encoding='utf-8') as f:
        for pair in qa_pairs:
            json.dump(pair, f, ensure_ascii=False)
            f.write('\n')

    print(f"ì´ {len(qa_pairs)}ê°œì˜ ì§ˆë¬¸-ë‹µë³€ ìŒì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ì‚¬ìš© ì˜ˆì‹œ
directory = 'data/Interview/Training/Labeling'  # ZIP íŒŒì¼ë“¤ì´ ìœ„ì¹˜í•œ ë””ë ‰í† ë¦¬ ê²½ë¡œ
zip_prefix = 'TL_'         # ì„ íƒí•  ZIP íŒŒì¼ë“¤ì˜ ì ‘ë‘ì‚¬
output_file = 'data/output.jsonl'          # ê²°ê³¼ë¥¼ ì €ì¥í•  íŒŒì¼ ê²½ë¡œ

process_zip_files(directory, zip_prefix, output_file)