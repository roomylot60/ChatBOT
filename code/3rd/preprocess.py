# âœ… preprocess.py (KoBERT ì „ìš©)
import os
import json
from tqdm import tqdm
from tokenization_kobert import KoBertTokenizer
from Korpora import Korpora

# ì„¤ì •
MAX_SEQ_LEN = 64
DATA_DIR = "data"
TOKENIZER_PATH = os.path.join(DATA_DIR, "tokenizer")
CORPUS_PATH = os.path.join(DATA_DIR, "corpus_tokenized.json")

# âœ… KoBERT í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = KoBertTokenizer.from_pretrained("monologg/kobert")
print("[INFO] âœ… KoBERT í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")
print(f"[INFO] ğŸ”¢ Vocab í¬ê¸°: {tokenizer.vocab_size}")
print(f"[INFO] ğŸ”  ì˜ˆì‹œ í† í°í™”: {tokenizer.tokenize('ê¸°ë¶„ì´ ì•ˆ ì¢‹ì•„')}")
print(f"[INFO] ğŸ”¤ CLS ID: {tokenizer.cls_token_id}, SEP ID: {tokenizer.sep_token_id}, PAD ID: {tokenizer.pad_token_id}")

# âœ… Korporaì—ì„œ ë°ì´í„° ë¡œë“œ
def process_korpora():
    print("[INFO] ğŸ“¦ Korpora ë°ì´í„° ë¡œë“œ ì‹œì‘")
    q_list, a_list = [], []

    try:
        Korpora.fetch("open_subtitles")
        open_subs = Korpora.load("open_subtitles")
        open_q, open_a = [], []
        for pair in tqdm(open_subs.train, desc="[LOAD open_subtitles]"):
            if '\t' in pair.text:
                q, a = pair.text.split('\t')
                open_q.append(q.strip())
                open_a.append(a.strip())
        q_list += open_q
        a_list += open_a
        print(f"[OK] âœ… open_subtitles ë¡œë“œ ì™„ë£Œ: {len(open_q)} ìŒ")
    except Exception as e:
        print(f"[FAIL] âŒ open_subtitles ë¡œë”© ì‹¤íŒ¨: {e}")

    try:
        nsmc = Korpora.load("nsmc")
        nsmc_q = [line.text.strip() for line in tqdm(nsmc.train, desc="[LOAD nsmc]")]
        nsmc_a = ["ì¢‹ì•„ìš”" if int(line.label) else "ë³„ë¡œì˜ˆìš”" for line in nsmc.train]
        q_list += nsmc_q
        a_list += nsmc_a
        print(f"[OK] âœ… NSMC ë¡œë“œ ì™„ë£Œ: {len(nsmc_q)} ìŒ")
    except Exception as e:
        print(f"[FAIL] âŒ NSMC ë¡œë”© ì‹¤íŒ¨: {e}")

    try:
        root_dir = os.path.abspath("data")
        modu = Korpora.load("modu_messenger", root_dir=root_dir)
        modu_q = [line.text.strip() for line in tqdm(modu.train, desc="[LOAD modu_messenger]")]
        modu_a = ["ì‘" for _ in modu_q]
        q_list += modu_q
        a_list += modu_a
        print(f"[OK] âœ… MODU ë¡œë“œ ì™„ë£Œ: {len(modu_q)} ìŒ")
    except Exception as e:
        print(f"[FAIL] âŒ MODU ë¡œë”© ì‹¤íŒ¨: {e}")

    print(f"[INFO] ì´ ìˆ˜ì§‘ëœ ë°ì´í„°: {len(q_list)} ìŒ")
    return q_list, a_list

# âœ… KoBERT í† í¬ë‚˜ì´ì§•
def tokenize_and_encode(q_list, a_list):
    input_encodings = []
    target_encodings = []
    for q, a in tqdm(zip(q_list, a_list), total=len(q_list), desc="[TOKENIZING]"):
        q_ids = tokenizer.encode(q, max_length=MAX_SEQ_LEN, padding="max_length", truncation=True)
        a_ids = tokenizer.encode(a, max_length=MAX_SEQ_LEN, padding="max_length", truncation=True)
        input_encodings.append({"input_ids": q_ids})
        target_encodings.append({"input_ids": a_ids})
    return input_encodings, target_encodings

# âœ… ì „ì²˜ë¦¬ ì‹¤í–‰
def save_preprocessed_data():
    q_list, a_list = process_korpora()

    if len(q_list) == 0:
        print("[ERROR] ìˆ˜ì§‘ëœ ë¬¸ì¥ì´ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return

    print("[INFO] âœ¨ í† í¬ë‚˜ì´ì§• ë° ì¸ì½”ë”© ì‹œì‘...")
    input_encodings, target_encodings = tokenize_and_encode(q_list, a_list)

    print(f"[INFO] ğŸ” ìƒ˜í”Œ ì¸ì½”ë”©:\nQ: {input_encodings[0]['input_ids']}\nA: {target_encodings[0]['input_ids']}")

    os.makedirs(DATA_DIR, exist_ok=True)
    with open(CORPUS_PATH, "w", encoding="utf-8") as f:
        json.dump({"input": input_encodings, "target": target_encodings}, f, ensure_ascii=False, indent=2)

    tokenizer.save_vocabulary(TOKENIZER_PATH)
    print(f"[âœ… ì™„ë£Œ] ì €ì¥ëœ ìŒ: {len(input_encodings)}ê°œ")
    print(f"[âœ… ì™„ë£Œ] í† í¬ë‚˜ì´ì € ì €ì¥ ê²½ë¡œ: {TOKENIZER_PATH}")
    print(f"[âœ… ì™„ë£Œ] ì½”í¼ìŠ¤ ì €ì¥ ê²½ë¡œ: {CORPUS_PATH}")

if __name__ == "__main__":
    save_preprocessed_data()
