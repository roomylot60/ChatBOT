# âœ… preprocess.py (Korpora ê¸°ë°˜)
import re
import os
import numpy as np
from tqdm import tqdm
from konlpy.tag import Okt
from gensim.models import Word2Vec
from Korpora import Korpora
import json

# ê¸°ë³¸ ì„¤ì •
FILTERS = "([~.,!?\"':;)(])"
PAD, SOS, END, UNK = "<PAD>", "<SOS>", "<END>", "<UNK>"
MARKER = [PAD, SOS, END, UNK]
MAX_SEQ = 25
okt = Okt()
CHANGE_FILTER = re.compile(FILTERS)

def load_korpora_corpus():
    print("[INFO] Korporaì—ì„œ ì±—ë´‡ ë° ë‹¤ë¥¸ ë§ë­‰ì¹˜ ë°ì´í„° ë¡œë“œ ì¤‘...")

    open_q, open_a = [], []
    nsmc_q, nsmc_a = [], []
    modu_q, modu_a = [], []

    try:
        Korpora.fetch("open_subtitles")
        open_subs = Korpora.load("open_subtitles")
        for pair in open_subs.train:
            if '\t' in pair.text:
                q, a = pair.text.split('\t')
                open_q.append(q.strip())
                open_a.append(a.strip())
        print(f"[OK] open_subtitles ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(open_q)}ìŒ")
    except Exception as e:
        print(f"[FAIL] open_subtitles ë¡œë”© ì‹¤íŒ¨: {e}")

    try:
        Korpora.fetch("nsmc")
        nsmc = Korpora.load("nsmc")
        nsmc_q = [pair.text.strip() for pair in nsmc.train]
        nsmc_a = ["ì¢‹ì•„ìš”" if int(pair.label) else "ë³„ë¡œì˜ˆìš”" for pair in nsmc.train]
        print(f"[OK] nsmc ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(nsmc_q)}ìŒ")
    except Exception as e:
        print(f"[FAIL] nsmc ë¡œë”© ì‹¤íŒ¨: {e}")

    try:
        korpora_root = os.path.abspath("data")
        modu = Korpora.load("modu_messenger", root_dir=korpora_root)
        modu_q = [line.text.strip() for line in modu.train]
        modu_a = ["ì‘" for _ in modu_q]
        print(f"[OK] modu_messenger ìˆ˜ë™ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(modu_q)}ìŒ")
    except Exception as e:
        print(f"[FAIL] modu_messenger ë¡œë”© ì‹¤íŒ¨: {e}")

    q_list = open_q + nsmc_q + modu_q
    a_list = open_a + nsmc_a + modu_a

    print(f"[INFO] ì´ ìˆ˜ì§‘ëœ ì§ˆë¬¸-ì‘ë‹µ ìŒ: {len(q_list)}")

    return q_list, a_list

def tokenize_morphs(sentence):
    return okt.morphs(re.sub(CHANGE_FILTER, "", sentence.replace(" ", "")))

def build_corpus(q_list, a_list):
    print("[DEBUG] build_corpus(): í˜•íƒœì†Œ ë¶„ì„ ì‹œì‘")
    corpus = []
    all_sentences = q_list + a_list

    for i, sent in enumerate(tqdm(all_sentences, desc="ğŸ§¼ Tokenizing")):
        tokens = tokenize_morphs(sent)
        if i < 3:  # ìƒ˜í”Œ 3ê°œ ì¶œë ¥
            print(f"[ìƒ˜í”Œ {i}] ì›ë¬¸: {sent}")
            print(f"[ìƒ˜í”Œ {i}] í˜•íƒœì†Œ: {tokens}")
        corpus.append(tokens)

    print(f"[INFO] ì „ì²´ ë¬¸ì¥ ìˆ˜: {len(corpus)} | ì˜ˆ: {corpus[0] if corpus else 'ì—†ìŒ'}")
    return corpus

def build_vocab(corpus):
    print("[DEBUG] build_vocab(): vocab ìƒì„± ì‹œì‘")
    vocab = {token: idx for idx, token in enumerate(MARKER)}

    for sentence in tqdm(corpus, desc="ğŸ”¤ Vocab Building"):
        for word in sentence:
            if word not in vocab:
                vocab[word] = len(vocab)

    print(f"[INFO] vocab í¬ê¸°: {len(vocab)}")
    print(f"[ìƒ˜í”Œ vocab] {list(vocab.items())[:10]}")
    return vocab

def save_vocab(vocab, path="vocab.json"):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(vocab, f, ensure_ascii=False, indent=4)

def load_vocab(path="vocab.json"):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)
    
def build_embedding_matrix(vocab, w2v_model, dim=300):
    embedding_matrix = np.random.normal(scale=0.6, size=(len(vocab), dim))
    for word, idx in vocab.items():
        if word in w2v_model.wv:
            embedding_matrix[idx] = w2v_model.wv[word]
    return embedding_matrix

def encode_sentences(sentences, vocab, max_seq_length=MAX_SEQ):
    output = []
    print("[INFO] encode_sentences ì§„í–‰ ì¤‘...")
    for s in tqdm(sentences, desc="Encoding"):
        tokens = tokenize_morphs(s)
        ids = [vocab.get(token, vocab[UNK]) for token in tokens][:max_seq_length]
        ids += [vocab[PAD]] * (max_seq_length - len(ids))
        output.append(ids)
    return np.array(output)

def decode_sentences(sentences, vocab, max_seq_length=MAX_SEQ):
    output = []
    print("[INFO] decode_sentences ì§„í–‰ ì¤‘...")
    for s in tqdm(sentences, desc="Decoding"):
        tokens = tokenize_morphs(s)
        ids = [vocab[SOS]] + [vocab.get(token, vocab[UNK]) for token in tokens][:max_seq_length - 1]
        ids += [vocab[PAD]] * (max_seq_length - len(ids))
        output.append(ids)
    return np.array(output)

def label_sentences(sentences, vocab, max_seq_length=MAX_SEQ):
    output = []
    print("[INFO] label_sentences ì§„í–‰ ì¤‘...")
    for s in tqdm(sentences, desc="Labeling"):
        tokens = tokenize_morphs(s)
        ids = [vocab.get(token, vocab[UNK]) for token in tokens][:max_seq_length - 1] + [vocab[END]]
        ids += [vocab[PAD]] * (max_seq_length - len(ids))
        output.append(ids)
    return np.array(output)