# # âœ… train.py (Korpora ê¸°ë°˜ + Word2Vec + Transformer)
# import os
# import json
# import torch
# import torch.nn as nn
# import torch.optim as optim
# from torch.utils.data import Dataset, DataLoader
# from tqdm import tqdm
# from KoBERT import KoBERT_Transformer
# from tokenization_kobert import KoBertTokenizer

# # ê²½ë¡œ
# DATA_DIR = "data"
# TOKENIZER_PATH = os.path.join(DATA_DIR, "tokenizer")
# CORPUS_PATH = os.path.join(DATA_DIR, "corpus_tokenized.json")
# MODEL_SAVE_PATH = "models/kobert_chatbot.pt"

# # ë””ë°”ì´ìŠ¤ ì„¤ì •
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# print(f"[INFO] âœ… ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

# # í† í¬ë‚˜ì´ì € ë¡œë“œ
# try:
#     tokenizer = KoBertTokenizer.from_pretrained("monologg/kobert")
#     print(f"[INFO] ğŸ”  í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ - Vocab Size: {tokenizer.vocab_size}")
#     print(f"[INFO] â›³ PAD ID: {tokenizer.pad_token_id}, CLS ID: {tokenizer.cls_token_id}, SEP ID: {tokenizer.sep_token_id}")
# except Exception as e:
#     print(f"[ERROR] âŒ í† í¬ë‚˜ì´ì € ë¡œë”© ì‹¤íŒ¨: {e}")
#     exit(1)

# PAD_ID = tokenizer.pad_token_id

# # ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ
# try:
#     with open(CORPUS_PATH, "r", encoding="utf-8") as f:
#         data = json.load(f)
#         input_data = data["input"]
#         target_data = data["target"]
#     print(f"[INFO] ğŸ“‚ ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ ì™„ë£Œ - ì´ ìƒ˜í”Œ ìˆ˜: {len(input_data)}")
# except Exception as e:
#     print(f"[ERROR] âŒ ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
#     exit(1)
    
# class ChatDataset(Dataset):
#     def __init__(self, data):
#         self.inputs = data["input"]
#         self.targets = data["target"]

#     def __len__(self): return len(self.inputs)

#     def __getitem__(self, idx):
#         return (
#             torch.tensor(self.inputs[idx]["input_ids"]),
#             torch.tensor(self.inputs[idx]["attention_mask"]),
#             torch.tensor(self.targets[idx]["input_ids"]),
#         )

# def collate_fn(batch):
#     q_ids, q_mask, a_ids = zip(*batch)
#     return (
#         nn.utils.rnn.pad_sequence(q_ids, batch_first=True, padding_value=tokenizer.pad_token_id),
#         nn.utils.rnn.pad_sequence(q_mask, batch_first=True, padding_value=0),
#         nn.utils.rnn.pad_sequence(a_ids, batch_first=True, padding_value=tokenizer.pad_token_id),
#     )

# dataset = ChatDataset(corpus)
# dataloader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)

# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# print(f"[INFO] í˜„ì¬ í•™ìŠµ ë””ë°”ì´ìŠ¤: {device}")
# model = KoBERT_Transformer(decoder_vocab_size=tokenizer.vocab_size).to(device)

# loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)
# optimizer = optim.Adam(model.parameters(), lr=5e-5)

# for epoch in range(3):
#     model.train()
#     total_loss = 0
#     print(f"\n[Epoch {epoch+1}] í•™ìŠµ ì‹œì‘")
#     for input_ids, attention_mask, target_ids in tqdm(dataloader):
#         input_ids, attention_mask, target_ids = input_ids.to(device), attention_mask.to(device), target_ids.to(device)
#         dec_input = target_ids[:, :-1]
#         dec_target = target_ids[:, 1:]

#         output = model(input_ids, attention_mask, dec_input)

#         loss = loss_fn(output.view(-1, output.size(-1)), dec_target.reshape(-1))
#         optimizer.zero_grad()
#         loss.backward()
#         optimizer.step()
#         total_loss += loss.item()
#     print(f"[Epoch {epoch+1}] í‰ê·  Loss: {total_loss / len(dataloader):.4f}")

# # ì €ì¥
# os.makedirs(os.path.dirname(MODEL_SAVE_PATH), exist_ok=True)
# torch.save(model.state_dict(), MODEL_SAVE_PATH)
# print("[âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ]")

# âœ… train.py (Korpora ê¸°ë°˜ + Word2Vec + Transformer)
import os
import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
from KoBERT import KoBERT_Transformer
from tokenization_kobert import KoBertTokenizer

# ê²½ë¡œ ì„¤ì •
DATA_DIR = "data"
TOKENIZER_PATH = os.path.join(DATA_DIR, "tokenizer")
CORPUS_PATH = os.path.join(DATA_DIR, "corpus_tokenized.json")
MODEL_SAVE_PATH = "models/kobert_chatbot.pt"

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"[INFO] âœ… ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

# í† í¬ë‚˜ì´ì € ë¡œë“œ
try:
    tokenizer = KoBertTokenizer.from_pretrained("monologg/kobert")
    print(f"[INFO] ğŸ”  í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ - Vocab Size: {tokenizer.vocab_size}")
    print(f"[INFO] â›³ PAD ID: {tokenizer.pad_token_id}, CLS ID: {tokenizer.cls_token_id}, SEP ID: {tokenizer.sep_token_id}")
except Exception as e:
    print(f"[ERROR] âŒ í† í¬ë‚˜ì´ì € ë¡œë”© ì‹¤íŒ¨: {e}")
    exit(1)

PAD_ID = tokenizer.pad_token_id

# ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ
try:
    with open(CORPUS_PATH, "r", encoding="utf-8") as f:
        data = json.load(f)
        input_data = data["input"]
        target_data = data["target"]
    print(f"[INFO] ğŸ“‚ ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ ì™„ë£Œ - ì´ ìƒ˜í”Œ ìˆ˜: {len(input_data)}")
except Exception as e:
    print(f"[ERROR] âŒ ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
    exit(1)

# ë°ì´í„°ì…‹ í´ë˜ìŠ¤
class ChatDataset(Dataset):
    def __init__(self, inputs, targets):
        self.inputs = inputs
        self.targets = targets

    def __len__(self): return len(self.inputs)

    def __getitem__(self, idx):
        return (
            torch.tensor(self.inputs[idx]["input_ids"]),
            torch.tensor(self.targets[idx]["input_ids"])
        )

# Collate í•¨ìˆ˜
def collate_fn(batch):
    input_ids, target_ids = zip(*batch)
    input_ids = nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=PAD_ID)
    target_ids = nn.utils.rnn.pad_sequence(target_ids, batch_first=True, padding_value=PAD_ID)
    return input_ids, target_ids

# ë°ì´í„°ë¡œë”
dataset = ChatDataset(input_data, target_data)
dataloader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)

# ëª¨ë¸ ì„ ì–¸
model = KoBERT_Transformer(decoder_vocab_size=tokenizer.vocab_size).to(device)
print(f"[INFO] âœ… ëª¨ë¸ êµ¬ì¡° ìƒì„± ì™„ë£Œ - ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")

# ì†ì‹¤í•¨ìˆ˜ & ì˜µí‹°ë§ˆì´ì €
loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_ID)
optimizer = optim.Adam(model.parameters(), lr=5e-5)

# í•™ìŠµ ë£¨í”„
for epoch in range(3):
    model.train()
    total_loss = 0
    print(f"\n[INFO] ğŸ” Epoch {epoch + 1} ì‹œì‘")

    for step, (input_ids, target_ids) in enumerate(tqdm(dataloader, desc=f"[TRAIN EPOCH {epoch+1}]")):
        input_ids, target_ids = input_ids.to(device), target_ids.to(device)

        enc_mask = (input_ids != PAD_ID).long()
        dec_input = target_ids[:, :-1]
        dec_target = target_ids[:, 1:]

        try:
            output = model(input_ids, enc_mask, dec_input)
            loss = loss_fn(output.view(-1, output.size(-1)), dec_target.reshape(-1))
        except Exception as e:
            print(f"[ERROR] âŒ ëª¨ë¸ ìˆœì „íŒŒ ì˜¤ë¥˜ at step {step}: {e}")
            continue

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

        # ìƒ˜í”Œ ë””ë²„ê¹… ì¶œë ¥ (1ê°œ ë°°ì¹˜ë§Œ)
        if step == 0:
            print(f"\n[DEBUG] ğŸ” ì²« ë°°ì¹˜ ìƒ˜í”Œ:")
            print("ğŸŸ© ì§ˆë¬¸ í† í° ID:", input_ids[0].tolist())
            print("ğŸŸ¦ ì‘ë‹µ(ì…ë ¥) í† í° ID:", dec_input[0].tolist())
            print("ğŸŸ¥ ì‘ë‹µ(íƒ€ê¹ƒ) í† í° ID:", dec_target[0].tolist())
            print("ğŸ§¾ ì˜ˆì‹œ ì§ˆë¬¸:", tokenizer.convert_ids_to_tokens(input_ids[0].tolist()))
            print("ğŸ§¾ ì˜ˆì‹œ ì‘ë‹µ ì…ë ¥:", tokenizer.convert_ids_to_tokens(dec_input[0].tolist()))

    avg_loss = total_loss / len(dataloader)
    print(f"[Epoch {epoch + 1}] ğŸ“‰ í‰ê·  Loss: {avg_loss:.4f}")

# ëª¨ë¸ ì €ì¥
os.makedirs(os.path.dirname(MODEL_SAVE_PATH), exist_ok=True)
torch.save(model.state_dict(), MODEL_SAVE_PATH)
print(f"[âœ… ì €ì¥ ì™„ë£Œ] ëª¨ë¸ì´ '{MODEL_SAVE_PATH}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
