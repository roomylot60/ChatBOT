# import os
# import json
# import torch
# import torch.nn as nn
# from tqdm import tqdm
# from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
# from rouge_score import rouge_scorer
# from KoBERT import KoBERT_Transformer
# from preprocess import CORPUS_PATH, TOKENIZER_PATH
# from tokenization_kobert import KoBertTokenizer
# # ì„¤ì •
# MAX_LEN = 64
# DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# print(f"[INFO] Device: {DEVICE}")

# tokenizer = KoBertTokenizer.from_pretrained("monologg/kobert")

# # ğŸ”¹ ëª¨ë¸ ë¡œë“œ
# model = KoBERT_Transformer(decoder_vocab_size=tokenizer.vocab_size).to(DEVICE)
# model.load_state_dict(torch.load("models/kobert_chatbot.pt", map_location=DEVICE))
# model.eval()

# def generate_response(input_text):
#     model.eval()
#     # ğŸ”¹ ì…ë ¥ ë¬¸ì¥ ë° í† í° í™•ì¸
#     print("[INPUT]", input_text)
#     token_ids = tokenizer.encode(input_text)
#     print("[ENCODED TOKEN IDS]", token_ids)
#     print("[ENCODED TOKENS]", tokenizer.convert_ids_to_tokens(token_ids))

#     enc = tokenizer.encode_plus(
#         input_text,
#         return_tensors="pt",
#         truncation=True,
#         max_length=MAX_LEN,
#         padding="max_length"
#     )

#     input_tensor = enc["input_ids"].to(DEVICE)
#     enc_mask = enc["attention_mask"].to(DEVICE)

#     START_TOKEN = tokenizer.cls_token_id
#     SEP_TOKEN = tokenizer.sep_token_id

#     dec_input = torch.tensor([[START_TOKEN]], dtype=torch.long).to(DEVICE)
#     result = []

#     for _ in range(MAX_LEN):
#         with torch.no_grad():
#             output = model(input_tensor, enc_mask, dec_input)
#         next_token = output.argmax(-1)[:, -1].item()
#         print(f"[DEBUG] next_token: {next_token} â†’ {tokenizer.convert_ids_to_tokens([next_token])}")
#         if next_token == SEP_TOKEN and len(result) > 1:
#             break
#         result.append(next_token)
#         dec_input = torch.cat([dec_input, torch.tensor([[next_token]]).to(DEVICE)], dim=1)

#     return tokenizer.decode(result, skip_special_tokens=True)

# print("\n[ë””ì½”ë”© í…ŒìŠ¤íŠ¸]")
# print("[MODEL PARAMS]", sum(p.numel() for p in model.parameters()))
# tokens = tokenizer.tokenize("ê¸°ë¶„ì´ ì•ˆ ì¢‹ì•„")
# print(tokens)
# ids = tokenizer.convert_tokens_to_ids(tokens)
# print(ids)
# response = generate_response("ê¸°ë¶„ì´ ì•ˆ ì¢‹ì•„")  # ê°ì •ì´ ëšœë ·í•œ ë¬¸ì¥
# print(f"ì‘ë‹µ: '{response}'")  # â† ì—¬ê¸° ê³µë€ì´ë©´ ë°”ë¡œ ìœ„ ë””ë²„ê¹… í¬ì¸íŠ¸ë¡œ ì ê²€

from tokenization_kobert import KoBertTokenizer

tokenizer = KoBertTokenizer.from_pretrained("monologg/kobert")
sample_text = "ì•ˆë…•í•˜ì„¸ìš”, ë°˜ê°‘ìŠµë‹ˆë‹¤."
tokens = tokenizer.tokenize(sample_text)
print(tokens)