# # evaluate.py
# import math
# import torch
# import torch.nn as nn
# import pandas as pd
# from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
# from rouge_score import rouge_scorer
# from gensim.models import Word2Vec
# from preprocess import load_data, build_corpus, build_vocab, encode_sentences, build_embedding_matrix
# from Transformer import Transformer

# # âš™ï¸ ì„¤ì •
# DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# print(f"âœ… Device: {DEVICE}")

# # ğŸ“Œ ë°ì´í„° ë¡œë”©
# q_list, a_list = load_data("../../data/ChatbotData.csv")
# corpus = build_corpus(q_list, a_list)
# vocab = build_vocab(corpus)
# w2v_model = Word2Vec.load("../../models/word2vec_ko.model")
# embedding_matrix = build_embedding_matrix(vocab, w2v_model, dim=300)

# # ğŸ”º ëª¨ë¸ ë¡œë“œ
# model = Transformer(
#     num_layers=4, d_model=300, num_heads=6, dff=512,
#     input_vocab_size=len(vocab), target_vocab_size=len(vocab),
#     embedding_matrix=embedding_matrix
# ).to(DEVICE)
# model.load_state_dict(torch.load("../../models/pretrained_transformer_model.pth", map_location=DEVICE))
# model.eval()

# loss_fn = nn.CrossEntropyLoss(ignore_index=vocab["<PAD>"])
# MAX_SEQ = 25

# # ğŸ”¹ ì‘ë‹µ ìƒì„± (Greedy)
# def generate_response(model, sentence, vocab):
#     model.eval()
#     input_ids = encode_sentences([sentence], vocab, max_seq_length=MAX_SEQ)
#     input_tensor = torch.tensor(input_ids[0]).unsqueeze(0).to(DEVICE)
    
#     dec_input = torch.tensor([[vocab["<SOS>"]]], dtype=torch.long).to(DEVICE)
#     for _ in range(MAX_SEQ):
#         output = model(input_tensor, dec_input)
#         next_token = output.argmax(-1)[:, -1].item()
#         if next_token == vocab["<END>"]:
#             break
#         dec_input = torch.cat([dec_input, torch.tensor([[next_token]]).to(DEVICE)], dim=1)

#     # idx2word ë³€í™˜
#     idx2word = {v: k for k, v in vocab.items()}
#     response = [idx2word.get(idx.item(), "<UNK>") for idx in dec_input[0][1:]]  # <SOS> ì œì™¸
#     return " ".join(response)

# # ğŸ”¹ BLEU ê³„ì‚°
# def calculate_bleu_score():
#     total_bleu = 0
#     total = 0
#     for q, a in zip(q_list, a_list):
#         reference = encode_sentences([a], vocab, max_seq_length=MAX_SEQ)[0]
#         candidate = encode_sentences([generate_response(model, q, vocab)], vocab, max_seq_length=MAX_SEQ)[0]
#         bleu = sentence_bleu([reference], candidate, smoothing_function=SmoothingFunction().method1)
#         total_bleu += bleu
#         total += 1
#     return total_bleu / total

# # ğŸ”¹ ROUGE ê³„ì‚°
# def calculate_rouge_score():
#     scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)
#     total_rouge1 = 0
#     total_rougeL = 0
#     total = 0
#     for q, a in zip(q_list, a_list):
#         prediction = generate_response(model, q, vocab)
#         scores = scorer.score(a, prediction)
#         total_rouge1 += scores['rouge1'].fmeasure
#         total_rougeL += scores['rougeL'].fmeasure
#         total += 1
#     return total_rouge1 / total, total_rougeL / total

# # ğŸ” ì‹¤í–‰
# print("\nğŸ” ì˜ˆì œ ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µ")
# samples = ["ì•ˆë…•", "ì˜¤ëŠ˜ ë­í•´?", "ë„ˆì˜ ì´ë¦„ì€ ë­ì•¼?", "ê¸°ë¶„ ì–´ë•Œ?"]
# for q in samples:
#     print(f"Q: {q}\nA: {generate_response(model, q, vocab)}\n")

# print("ğŸ” ì„±ëŠ¥ í‰ê°€ ì‹œì‘...")
# bleu = calculate_bleu_score()
# rouge1, rougeL = calculate_rouge_score()
# print(f"âœ… BLEU Score: {bleu:.4f}")
# print(f"âœ… ROUGE-1 F1: {rouge1:.4f}")
# print(f"âœ… ROUGE-L F1: {rougeL:.4f}")

import torch
from preprocess import load_data, build_corpus, build_vocab, build_embedding_matrix, encode_sentences
from gensim.models import Word2Vec
from Transformer import Transformer
import pandas as pd

# ì„¤ì •
MAX_SEQ = 25
EMBED_DIM = 300
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ë°ì´í„° ë° ì‚¬ì „ ë¡œë“œ
q_list, a_list = load_data("../../data/ChatbotData.csv")
corpus = build_corpus(q_list, a_list)
vocab = build_vocab(corpus)

# âœ… ë””ë²„ê¹… ì¶œë ¥
print(f"[INFO] ì´ vocab í¬ê¸°: {len(vocab)}")

# Word2Vec ë¡œë“œ
w2v_path = "../../models/word2vec_ko.model"
try:
    w2v_model = Word2Vec.load(w2v_path)
    print(f"[INFO] Word2Vec ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {w2v_path}")
except Exception as e:
    print(f"[ERROR] Word2Vec ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    exit()

# ì„ë² ë”© ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
embedding_matrix = build_embedding_matrix(vocab, w2v_model, dim=EMBED_DIM)

# Transformer ëª¨ë¸ ìƒì„±
model = Transformer(
    num_layers=4,
    d_model=EMBED_DIM,
    num_heads=6,
    dff=512,
    input_vocab_size=len(vocab),
    target_vocab_size=len(vocab),
    embedding_matrix=embedding_matrix
).to(DEVICE)

# ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ
model_path = "../../models/pretrained_transformer_model.pth"
try:
    model.load_state_dict(torch.load(model_path, map_location=DEVICE))
    model.eval()
    print(f"[INFO] Transformer ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_path}")
except Exception as e:
    print(f"[ERROR] Transformer ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    exit()

# idx2word ë§¤í•‘
idx2word = {v: k for k, v in vocab.items()}

# ì±—ë´‡ ì‘ë‹µ ìƒì„± í•¨ìˆ˜ (ë””ë²„ê¹…ìš©)
def generate_response(model, sentence, vocab, max_seq=MAX_SEQ):
    input_ids = encode_sentences([sentence], vocab, max_seq_length=max_seq)
    input_tensor = torch.tensor(input_ids[0], dtype=torch.long).unsqueeze(0).to(DEVICE)

    dec_input = torch.tensor([[vocab["<SOS>"]]], dtype=torch.long).to(DEVICE)
    output_tokens = []

    print(f"[DEBUG] ì§ˆë¬¸: {sentence}")
    print(f"[DEBUG] ì…ë ¥ í† í°: {input_ids[0]}")

    output_tokens = []

    for i in range(max_seq):
        with torch.no_grad():
            output = model(input_tensor, dec_input)  # (1, t, vocab_size)
            probs = torch.softmax(output[:, -1, :], dim=-1)
            topk = torch.topk(probs, k=5)

            print(f"[STEP {i}] Top-5 ë‹¤ìŒ í† í°: {[(idx.item(), idx2word.get(idx.item(), '<UNK>'), prob.item()) for idx, prob in zip(topk.indices[0], topk.values[0])]}")

            next_token = torch.argmax(probs[0, :]).item()

        # âœ… ì²« í† í°ì´ <END>ì´ë©´ ë¬´ì‹œí•˜ê³  ì§„í–‰
        if next_token == vocab["<END>"]:
            if i == 0:
                print(f"[STEP {i}] <END> ì˜ˆì¸¡ë˜ì—ˆìœ¼ë‚˜ ì²« step â€” ë¬´ì‹œí•˜ê³  ê³„ì†")
            else:
                print(f"[STEP {i}] <END> ì˜ˆì¸¡ë¨ â€” ì¢…ë£Œ")
                break

        output_tokens.append(next_token)

        # ë””ì½”ë” ì…ë ¥ì— ë‹¤ìŒ í† í° ì¶”ê°€
        next_token_tensor = torch.tensor([[next_token]], dtype=torch.long).to(DEVICE)
        dec_input = torch.cat([dec_input, next_token_tensor], dim=1)



    decoded_words = [idx2word.get(token, "<UNK>") for token in output_tokens]

    print(f"[DEBUG] ì¶œë ¥ í† í°: {output_tokens}")
    print(f"[DEBUG] ì‘ë‹µ ë‹¨ì–´: {decoded_words}")

    return " ".join(decoded_words)

# í…ŒìŠ¤íŠ¸
sample_questions = ["ì•ˆë…•", "ë„ˆ ë­í•´", "ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?"]

for q in sample_questions:
    print(f"Q: {q}")
    answer = generate_response(model, q, vocab)
    print(f"A: {answer}\n")

for name, param in model.named_parameters():
    if "embedding" in name:
        print(f"[CHECK] {name} í‰ê· ê°’: {param.data.mean().item():.4f}, std: {param.data.std().item():.4f}")

print(f"[CHECK] embedding.weight í‰ê· ê°’: {model.embedding.weight.data.mean().item():.4f}, std: {model.embedding.weight.data.std().item():.4f}")